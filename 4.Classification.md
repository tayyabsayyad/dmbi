1. A table below shows a sample dataset of whether a customer responds to the survey or not. “Outcome” is a class label. Construct a Decision tree classifier for the dataset. For a new example ( Rural, semidetached, low, No)  what will be the predicted class.

![Table 4.1](/Images/Table_4.1.png)

2. Brifly explain Bagging and Boosting of the Classfiers
a) Bagging :

  Given a set, D, of d tuples, bagging works as follows. For iteration i .i D 1, 2, : : : , k/, a training set, Di , of d tuples is sampled with replacement from the original set of tuples, D. Note that the term bagging stands for bootstrap aggregation. Each training set is a bootstrap sample, as described in Section 8.5.4. Because sampling with replacement is used, some of the original tuples of D may not be included in Di , whereas others may occur more than once. A classifier model, Mi , is learned for each training set, Di .To classify an unknown tuple, X, each classifier, Mi , returns its class prediction, which counts as one vote. The bagged classifier, M, counts the votes and assigns the class with the most votes to X. Bagging can be applied to the prediction of continuous values by taking the average value of each prediction for a given test tuple. The algorithm is summarized below.
  The bagged classifier often has significantly greater accuracy than a single classifier derived from D, the original training data. It will not be considerably worse and is more robust to the effects of noisy data and overfitting. The increased accuracy occurs because the composite model reduces the variance of the individual classifiers.

Algorithm: Bagging. The bagging algorithm—create an ensemble of classification models
for a learning scheme where each model gives an equally weighted prediction.
Input:

D, a set of d training tuples;
k, the number of models in the ensemble;
a classification learning scheme (decision tree algorithm, na¨ıve Bayesian, etc.).

Output: The ensemble—a composite model, M.
Method:
(1) for i D 1 to k do // create k models:
(2)   create bootstrap sample, Di , by sampling D with replacement;
(3)   use Di and the learning scheme to derive a model, Mi ;
(4) endfor

b) Boosting :

  In boosting, weights are also assigned to each training tuple. A series of k classifiers is iteratively learned. After a classifier, Mi , is learned, the weights are updated to allow the subsequent classifier,MiC1, to “pay more attention” to the training tuples that were misclassified by Mi . The final boosted classifier, M, combines the votes of each individual classifier, where the weight of each classifier’s vote is a function of its accuracy. 
  AdaBoost (short for Adaptive Boosting) is a popular boosting algorithm. Suppose we want to boost the accuracy of a learning method. We are given D, a data set of d class-labeled tuples, .X1, y1/, .X2, y2/, : : : , .Xd, yd/, where yi is the class label of tuple Xi. Initially, AdaBoost assigns each training tuple an equal weight of 1=d. Generating k classifiers for the ensemble requires k rounds through the rest of the algorithm. In round i, the tuples from D are sampled to form a training set, Di , of size d. Sampling with replacement is used—the same tuple may be selected more than once. Each tuple’s chance of being selected is based on its weight. A classifier model, Mi , is derived from the training tuples of Di . Its error is then calculated using Di as a test set. The weights of the training tuples are then adjusted according to how they were classified. If a tuple was incorrectly classified, its weight is increased. If a tuple was correctly classified, its weight is decreased. A tuple’s weight reflects how difficult it is to classify— the higher the weight, the more often it has been misclassified. These weights will be used to generate the training samples for the classifier of the next round. The basic idea is that when we build a classifier, we want it to focus more on the misclassified tuples of the previous round. Some classifiers may be better at classifying some “difficult” tuples than others. In this way, we build a series of classifiers that complement each other.
  
                     error(Mi) = summation(wj) * err(Xj) [for j= 1 to d]
                     
where err.Xj/ is the misclassification error of tuple Xj: If the tuple was misclassified, then err.Xj/ is 1; otherwise, it is 0. If the performance of classifier Mi is so poor that its error exceeds 0.5, then we abandon it. Instead, we try again by generating a new Di training set, from which we derive a new Mi . The weight of classifier Mi ’s vote is 

                     log(1-error(Mi)/error(Mi))

Algorithm: 
A boosting algorithm—create an ensemble of classifiers. Each one
gives a weighted vote.

Input:
D, a set of d class-labeled training tuples;
k, the number of rounds (one classifier is generated per round);
a classification learning scheme.

Output: A composite model.
Method:
(1) initialize the weight of each tuple in D to 1=d;
(2) for i D 1 to k do // for each round:
(3)   sample D with replacement according to the tuple weights to obtain Di ;
(4)   use training set Di to derive a model, Mi ;
(5)   compute error.Mi/, the error rate of Mi (Eq. 8.34)
(6)   if error.Mi/ > 0.5 then
(7)     go back to step 3 and try again;
(8)   endif
(9)   for each tuple in Di that was correctly classified do
(10)    multiply the weight of the tuple by error.Mi/=.1􀀀error.Mi//; // update weights
(11)    normalize the weight of each tuple;
(12)  endfor

3. Define Classification, Issues of Classification and Explain ID3 classfication with example.


4. Explain different methods that can be used to evaluate and compare accuracy of different classfication algorthms.


5. Use any classification techniques and find out the class of (Homeowner = Yes, Status = Employed , Income = Average)

ID,Homeowner,Status,Income, Defaulted
1,Yes,Employed,High,No,
2,No,Business,Average,NO
3,No,Employed,Low,No
4,Yes,Business,High,No
5,No,Unemployed,Average,Yes
6,No,Business,Low,No
7,Yes,Unemployed,High,No
8,No,Employed,Average,Yes
9,No,Business,Low,No
10,No,Employed,Average,Yes



